{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Building_BERT_model.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vizzies/NASA-Semantic-Search-Engine-for-Scientific-Literature/blob/master/previous_ml_models/Building-BERT-model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciRP6XqLrHP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(‘/content/gdrive’)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2N9_60snU7R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6159b7f6-eaae-4e5d-e4c6-161ba2552063"
      },
      "source": [
        "# GPU Setup\n",
        "\n",
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB4DwkIwodyy",
        "colab_type": "text"
      },
      "source": [
        "# Import Data Below and Parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFD53HqLnYoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas\n",
        "\n",
        "with open('/content/arc-code-ti-publications.pkl', 'rb') as f:\n",
        "    pubs = pandas.read_pickle(f)\n",
        "\n",
        "import re\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)\n",
        "    \n",
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "full_texts = []\n",
        "sentences = list(pubs['Text'])\n",
        "for sen in sentences:\n",
        "    full_texts.append(preprocess_text(str(sen)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiK4xOKFoypg",
        "colab_type": "text"
      },
      "source": [
        "## Bert Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBX1Q3Kfo1Mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsayLg-xo6y9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply tokenizer\n",
        "\n",
        "# Print the original sentence.\n",
        "print(' Original: ', sentences[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBNrUX9uvwNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tokenize entire dataset\n",
        "\n",
        "max_len = 0\n",
        "\n",
        "# For every sentence...\n",
        "for sent in sentences:\n",
        "\n",
        "    # Tokenize the text and add CLS and SEP tokens.\n",
        "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNlxZnv-vzOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for sent in sentences:\n",
        "  encoded_dict = tokenizer.encode_plus(\n",
        "      sent,\n",
        "      add_special_toekns = True, # adds CLS and SEP tokens\n",
        "      max_length = 64,\n",
        "      pad_to_max_length = True,\n",
        "      return_attention_mask = True, # Creates attention masks\n",
        "      return_tensors = 'pt', #pytorch\n",
        "  )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# convert lists to tensors\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "print('Original: ', sentences[0])\n",
        "print('Token IDs:', input_ids[0])gr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOf58Su03fbA",
        "colab_type": "text"
      },
      "source": [
        "## Split dataset\n",
        "90% for training, 10% for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Vil7qw3mrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "train_size = int(0.9 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyK2ABQW_VPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset iterator to save memory during training\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# 16 or 32 is ok\n",
        "batch_size = 32\n",
        "\n",
        "# dataloaders for training and validation in random order\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "# sequential reading for validation\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset,\n",
        "            sampler = SequentialSampler(val_dataset), \n",
        "            batch_size = batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vzp4mrNSW51a",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERVDrzXgW7v0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "import numpy as np\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q8iaWFvXDAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# akes a time in seconds and returns a string hh:mm:ss\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "io7X1vjOJG9D",
        "colab_type": "text"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X603NeDJKOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # uncased vocab 12 layer model\n",
        "    num_labels = 2, \n",
        "    output_attentions = False, \n",
        "    output_hidden_states = False, \n",
        ")\n",
        "\n",
        "# run on gpu\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6CDtFVcVo_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# all model parameters (i.e. embedding layers, weights, bias, encoder layers, etc)\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAZncZ8BWEgd",
        "colab_type": "text"
      },
      "source": [
        "# Optimizer and LR Scheduler\n",
        "AdamW optimizer [here](https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L109) in `runglue.py`\n",
        "\n",
        "* Batch size: 32 (set in DataLoaders)\n",
        "* Learning rate: 2e-5\n",
        "* Epochs: 4 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTn63YfWWKGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzDOObbxWm70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# amt of training epochs, keep between 2 - 4\n",
        "epochs = 4\n",
        "\n",
        "# [number of batches] x [number of epochs]\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# learning rate scheduler\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, \n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgZ1HbKNXOaq",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    }
  ]
}